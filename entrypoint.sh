#!/bin/bash
set -e

# Enhanced cuDNN and CUDA environment setup
export CUDNN_LOGINFO_DBG=${CUDNN_LOGINFO_DBG:-0}
export CUDNN_LOGERR_DBG=${CUDNN_LOGERR_DBG:-1}
export CUDNN_LOGWARN_DBG=${CUDNN_LOGWARN_DBG:-1}
export CUDA_LAUNCH_BLOCKING=${CUDA_LAUNCH_BLOCKING:-0}
export PYTORCH_CUDA_ALLOC_CONF=${PYTORCH_CUDA_ALLOC_CONF:-"max_split_size_mb:512,expandable_segments:True"}
export CUDA_MODULE_LOADING=${CUDA_MODULE_LOADING:-LAZY}
export TORCH_CUDNN_V8_API_ENABLED=${TORCH_CUDNN_V8_API_ENABLED:-1}

# Create CUDA cache directory if it doesn't exist
mkdir -p /tmp/cuda_cache
export CUDA_CACHE_PATH=/tmp/cuda_cache




echo "ğŸ–¥ï¸  === Environment Setup entrypoint.sh ==="
echo "ğŸ”§  CUDA_LAUNCH_BLOCKING: $CUDA_LAUNCH_BLOCKING"
echo "ğŸ”§  PYTORCH_CUDA_ALLOC_CONF: $PYTORCH_CUDA_ALLOC_CONF"
echo "ğŸ”§  CUDNN_LOGINFO_DBG: $CUDNN_LOGINFO_DBG"
echo "ğŸ”§  TORCH_CUDNN_V8_API_ENABLED: $TORCH_CUDNN_V8_API_ENABLED"
echo "ğŸ–¥ï¸  ======================================="

# Critical dependency test - fail fast if imports don't work
echo "ğŸ” Testing critical dependencies..."
python -c "
import sys
try:
    import numpy
    print(f'âœ… NumPy {numpy.__version__} imported successfully')

    import scipy
    print(f'âœ… SciPy {scipy.__version__} imported successfully')

    from scipy import special
    print('âœ… SciPy special module imported successfully')

    import torch
    print(f'âœ… PyTorch {torch.__version__} imported successfully')

    from pyannote.audio import Pipeline
    print('âœ… pyannote.audio imported successfully')

    print('ğŸ‰ All critical dependencies imported successfully!')

except Exception as e:
    print(f'âŒ CRITICAL IMPORT ERROR: {e}')
    print('ğŸš¨ Container will exit to prevent restart loop')
    sys.exit(1)
"

# Check if CUDA is available (informational, with unique emoticons)
python -c "
import torch
print('ğŸ¦„ğŸ”¸ PyTorch version:', torch.__version__)
print('ğŸ¦„ğŸ”¸ CUDA available:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('ğŸ¦„ğŸ”¸ CUDA version:', torch.version.cuda)
    print('ğŸ¦„ğŸ”¸ cuDNN enabled:', torch.backends.cudnn.enabled)
    try:
        print('ğŸ¦„ğŸ”¸ cuDNN version:', torch.backends.cudnn.version())
    except:
        print('ğŸ¦„ğŸ”¸ cuDNN version: Unable to determine')
    print('ğŸ¦„ğŸ”¸ GPU count:', torch.cuda.device_count())
    if torch.cuda.device_count() > 0:
        print('ğŸ¦„ğŸ”¸ GPU name:', torch.cuda.get_device_name(0))
else:
    print('ğŸ¦„ğŸ”¸ CUDA not available - will use CPU')
" 2>/dev/null || echo "ğŸ¦„ğŸ”¸ Unable to check CUDA status"

# Ensure proper permissions for directories (process one at a time with progress)
echo "ğŸ”§ Setting up directory permissions..."

for dir in "/data/in" "/data/out" "/data/backup" "/tmp/transcribe" "/tmp/cuda_cache"; do
    if [ -d "$dir" ]; then
        echo "ğŸ”§ Processing directory: $dir"
        file_count=$(find "$dir" -type f 2>/dev/null | wc -l || echo "unknown")
        echo "ğŸ”§ Files to process: $file_count"
        sudo chown -R transcribe:transcribe "$dir" 2>/dev/null || echo "âš ï¸  Failed to change ownership of $dir"
        echo "âœ… Completed: $dir"
    else
        echo "âš ï¸  Directory not found: $dir"
    fi
done

echo "ğŸ”§ Directory permissions setup complete"



# Execute the main command
exec "$@"
