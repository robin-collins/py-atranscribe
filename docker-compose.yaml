# Docker Compose configuration for py-atranscribe
# Automated Audio Transcription with Speaker Diarization

name: py-atranscribe

services:
  insanely-fast-whisper-api:
    image: yoeven/insanely-fast-whisper-api:latest
    container_name: ifw-api
    hostname: ifw-api
    restart: "unless-stopped"
    ports:
      - "9000:9000"
    env_file:
      - .env
#    volumes:
#      - transcribe-cache:/home/transcribe/.cache
#      - transcribe-models:/root/.cache  # For model caching
    expose:
      - "9000"
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 2G

#    GPU support (uncomment if using GPU)
#    deploy:
#      resources:
#        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  transcriber:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: py-atranscribe
    restart: "unless-stopped"  # Disable automatic restart to allow proper failure handling

    env_file:
      - .env

    # Volume mounts
    volumes:
#      - /mnt/shared/raspi-audio:/data/in
      - ./temp/in:/data/in

#      - /mnt/shared/raspi-audio/audio/output:/data/out
#      - /mnt/shared/transcribed-audio:/data/out
      - ./temp/out:/data/out

#      - /mnt/shared/raspi-audio/audio/backup:/data/backup
#      - /mnt/shared/raspi-audio-backup:/data/backup
      - ./temp/backup:/data/backup
      - ./config.yaml:/app/config.yaml:ro  # Mount config as read-only
      - ./logs/transcribe.log:/var/log/transcribe.log:rw
      - transcribe-cache:/home/transcribe/.cache
      - transcribe-models:/root/.cache  # For model caching

    # Port mapping for health checks
    ports:
      - "8000:8000"  # Health check endpoint

    # Resource limits
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 2G

#    GPU support (uncomment if using GPU)
#    deploy:
#      resources:
#        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 5s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

# Named volumes
volumes:
  transcribe-cache:
    driver: local
  transcribe-models:
    driver: local

# Networks
networks:
  default:
    name: py-atranscribe-network
